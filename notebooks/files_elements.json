{
    "CSC311f19_final": [
        "Last (Family) Name:",
        "First (Given) Name:",
        "Student Number:",
        "Section (circle one):",
        "L0101 = Mon,",
        "L0201 = Wed,",
        "L0301 = Th",
        "PRACTICE FINAL EXAM",
        "CSC311 Fall 2019 Introduction to Machine Learning",
        "University of Toronto Faculty of Arts & Science",
        "Duration - 3 hours Aids allowed: Two double-sided handwritten 8.5(cid:48)(cid:48) \u00d7 11(cid:48)(cid:48) or A4 aid sheets.",
        "Exam reminders:",
        "Fill out your name and student number on the top of this page. \u2022 Do not begin writing the actual exam until the announcements have ended and the Exam",
        "Write all answers only in the space provided after each question. Last few pages are provided for scratch work. They won\u2019t be graded.",
        "Write all answers only in the space provided after each question. Last few pages are provided for scratch work. They won\u2019t be graded.",
        "Blank scrap paper is provided at the back of the exam. \u2022 If you possess an unauthorized aid during an exam, you may be charged with an academic o\ufb00ence.",
        "Turn o\ufb00 and place all cell phones, smart watches, electronic devices, and unauthorized study materials in your bag under your desk. If it is left in your pocket, it may be an academic o\ufb00ence.",
        "When you are done your exam, raise your hand for someone to come and collect your exam. Do not collect your bag and jacket before your exam is handed in.",
        "If you are feeling ill and unable to \ufb01nish your exam, please bring it to the attention of an Exam Facilitator so it can be recorded before leaving the exam hall.",
        "In the event of a \ufb01re alarm, do not check your cell phone when escorted outside.",
        "Hand in all examination materials at the end DO NOT WRITE ANY ANSWERS ON THIS PAPER",
        "1",
        "1. True/False. For each statement below, say whether it is true or false, and give a one or two sentence justi\ufb01cation of your answer.",
        "a) Adding more training data always reduces over\ufb01tting.",
        "b) For small k, the k-means algorithm is equivalent to the k-nearest neighbors algorithm.",
        "c) An ensemble of models always has more capacity than a single model.",
        "d) A linear SVM will \ufb01nd the same decision boundary as logistic regression.",
        "2",
        "2. Reinforcement Learning.",
        "1",
        "2",
        "3",
        "4",
        "A",
        "+10",
        "B",
        "10",
        "C",
        "Consider the familiar robot navigation task within the gridworld shown above. You can move in any of the four directions (left/right/up/down) unless blocked by one of the gray obstacles at B2 and B3. The rewards are +10 for state C4, and -10 for state B4. A4 and B4 are both absorbing states. The reward for every other state is 0.",
        "a) Assume that the state transitions are deterministic. Recall that under the simple Q-learning algorithm, the estimate Q values are update using the following rule:",
        "\u02c6Q(s, a) = r(s(cid:48)) + \u03b3 max",
        "a(cid:48)",
        "\u02c6Q(s(cid:48), a(cid:48))",
        "Consider applying this algorithm when all the \u02c6Q values are initialized to zero and \u03b3 = 0.8. Write the Q estimates on the \ufb01gure as labeled arrows after the robot has executed the following state sequences:",
        "B1 \u2192 A2 \u2192 A2 \u2192 A3 \u2192 B3 \u2192 B4 \u2022 A2 \u2192 A3 \u2192 A4 \u2022 C1 \u2192 C2 \u2192 C3 \u2192 B3 \u2192 A3 \u2192 A4",
        "3",
        ".",
        "b) Assume the robot will now use the policy of always performing the action having the greatest Q value. Is this the optimal policy? Why or why not?",
        "c) Suppose state A3 also has a reward of -10. How can we ensure that our agent is still able to \ufb01nd the optimal policy in this new environment?",
        "4",
        "3. Backpropagation. Consider a L2 regularized single layer neural network model that pre- dicts continuous 1d targets y = \u03c3(z) \u2208 R where z = wh + b and h = \u03c3(w(cid:48)x + b(cid:48)), and \u03c3 is an activation function. To train, we use mean squared error from the targets with L2 penalty on t \u2208 R : L = (y \u2212 t)2/2 + w2 + b(cid:48)2 \u2202w , \u2202L \u2202b(cid:48) .",
        "b) Now compute \u2202L",
        "\u2202w , \u2202L",
        "\u2202b(cid:48) using the backprogation algorithm.",
        "c) What are the disadvantages of doing a) versus backpropagation? Why do we use backpropa- gation in machine learning as opposed to direct di\ufb00erentiation?",
        "5",
        "4. Principal Component Analysis. Recall that the optimal PCA subspace can be deter- mined from the eigendecomposition of the empirical covariance matrix \u02c6\u03a3. Also recall that the eigendecomposition can be expressed in terms of the following spectral decomposition of \u02c6\u03a3: \u02c6\u03a3 = Q\u039bQ(cid:62),",
        "where Q is an orthogonal matrix and \u039b is a diagonal matrix. Assume the eigenvalues are sorted from largest to smallest. You may assume all of the eigenvalues are distinct.",
        "1. If you\u2019ve already computed the eigendecomposition (i.e. Q and \u039b), how do you obtain the orthogonal basis U for the optimal PCA subspace? (You do not need to justify your answer.)",
        "2. The PCA code vector for a data point x is given by z = U(cid:62)(x \u2212 \u02c6\u00b5) where \u02c6\u00b5 is the data mean. Show that the entries of z are uncorrelated.",
        "6",
        "5. Support Vector Machines. Support vector machines learn a decision boundary leading to the largest margin from both classes. You are training SVM on a tiny dataset with 4 points shown below. This dataset consists of two examples with class label -1 (denoted with plus), and two examples with class label +1 (denoted with triangles).",
        "a) Write down the SVM loss function for this data and state how to \ufb01nd the weight vector w and bias b.",
        "b) Draw the (approximate) decision boundary.",
        "7",
        "6. Probabilistic Models . The Laplace distribution, parameterized by \u00b5 and \u03b2, is de\ufb01ned as follows:",
        "Laplace(w; \u00b5, \u03b2) =",
        "1 2\u03b2",
        "exp",
        "(cid:18)",
        "\u2212",
        "|w \u2212 \u00b5| \u03b2",
        "(cid:19) .",
        "Consider a variant of the homework2 question where we assume that the prior over the weights w consists of an independent zero-centered Laplace distribution for each dimension, with shared parameter \u03b2:",
        "wj \u223c Laplace(0, \u03b2) t | w \u223c N (t; w(cid:62)x, \u03c32)",
        "For reference, the Gaussian PDF is:",
        "N (x; \u00b5, \u03c3) =",
        "\u221a",
        "1 2\u03c0\u03c3",
        "exp",
        "(cid:18)",
        "\u2212",
        "(x \u2212 \u00b5)2 2\u03c32",
        "(cid:19) .",
        "1. Suppose you have a labeled training set {(x(i), t(i))}N minimize to \ufb01nd the MAP estimate of w.",
        "1. Suppose you have a labeled training set {(x(i), t(i))}N minimize to \ufb01nd the MAP estimate of w.",
        "2. Based on your answer to part (a), how might the MAP solution for a Laplace prior di\ufb00er from the MAP solution if you use a Gaussian prior (which is exactly homework2)?",
        "8",
        "7. EM Algorithm.",
        "1. Is EM algorithm a supervised or an unsupervised learning method? Explain your answer.",
        "2. How does EM algorithm and k-means compare? Write 3 similarities and 3 di\ufb00erences.",
        "3. Explain why we call these steps expectation and maximization steps. What is it that we take expectation of and what is it that we maximize?",
        "9",
        "SCRATCH WORK ONLY: THIS PAGE WILL NOT BE GRADED",
        "1",
        "2",
        "3",
        "A",
        "B",
        "C",
        "1",
        "2",
        "3",
        "A",
        "B",
        "C",
        "10",
        "4",
        "+10",
        "10",
        "4",
        "+10",
        "10",
        "SCRATCH WORK ONLY: THIS PAGE WILL NOT BE GRADED",
        "11",
        "SCRATCH WORK ONLY: THIS PAGE WILL NOT BE GRADED",
        "12",
        "SCRATCH WORK ONLY: THIS PAGE WILL NOT BE GRADED",
        "13",
        "SCRATCH WORK ONLY: THIS PAGE WILL NOT BE GRADED",
        "14"
    ],
    "CSC411f18_midterm2": [
        "Midterm for CSC411/2515, Machine Learning and Data Mining Fall 2018, Version A Friday, October 19, 6:10-7pm",
        "Name:",
        "Student number:",
        "This is a closed-book test. It is marked out of 15 marks. Please answer",
        "ALL of the questions. Here is some advice:",
        "The questions are NOT arranged in order of di\ufb03culty, so you should attempt every question.",
        "Questions that ask you to \u201cbrie\ufb02y explain\u201d something only require short (1-3 sentence) explanations. Don\u2019t write a full page of text. We\u2019re just looking for the main idea.",
        "None of the questions require long derivations. If you \ufb01nd yourself plug- ging through lots of equations, consider giving less detail or moving on to the next question.",
        "Many questions have more than one right answer.",
        "CSC411/2515 Fall 2018",
        ".",
        "Q1: Q2: Q3: Q4: Q5: Q6: Q7: Q8: Q9:",
        "Final mark:",
        "2",
        "/ 1 / 2 / 1 / 1 / 2 / 2 / 2 / 2 / 2",
        "/ 15",
        "Midterm Test, Version A",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "Have you taken CSC321 at UofT? (This question is used for calibration purposes.)",
        "1. [1pt] Give one reason why an algorithm implemented in terms of matrix and vector operations can be faster than the same algorithm implemented in terms of for-loops.",
        "2. [2pts] Brie\ufb02y explain two advantages of decision trees over K-nearest-neighbors.",
        "3. [1pt] TRUE or FALSE: AdaBoost will eventually choose a weak classi\ufb01er that achieves a weighted error rate of 0. Brie\ufb02y justify your answer.",
        "3",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "4. [1pt] In class, we considered using the squared Euclidean norm of the weights, (cid:107)w(cid:107)2, as a regularizer. Suppose we instead used the Euclidean norm (cid:107)w(cid:107) as a regularizer (i.e. without squaring it). Brie\ufb02y explain one way in which this would lead to di\ufb00erent behavior.",
        "5. [2pts] Recall that hyperparameters are often tuned using a validation set.",
        "(a) [1pt] Give an example of a hyperparameter which it is OK to tune on the training",
        "set (rather than a validation set). Brie\ufb02y explain your answer.",
        "(b) [1pt] Give an example of a hyperparameter which should be tuned on a validation",
        "set, rather than the training set. Brie\ufb02y explain your answer.",
        "4",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "6. [2pts] In this question, you will write NumPy code to implement a 1-nearest-neighbour classi\ufb01er. Assume you are given an N \u00d7 D data matrix X, where each row corresponds to one of the input vectors, and an integer array y with the corresponding labels. (You may assume the labels are integers from 1 to K.) You are given a query vector x_query. Your job is to return the predicted class (as an integer). Do not use a for-loop. If you don\u2019t remember the API for a NumPy operation, then for partial credit, explain what you are trying to do.",
        "5",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "7. [2pts] Consider the classi\ufb01cation problem with the following dataset:",
        "x1 x2 x3 0 0 0 0 1 0 1 1 0 1 1 1",
        "t 1 0 1 0",
        "Your job is to \ufb01nd a linear classi\ufb01er with weights w1, w2, w3, and b which correctly classi\ufb01es all of these training examples. None of the examples should lie on the decision boundary.",
        "(a) [1pt] Give the set of linear inequalities the weights and bias must satisfy.",
        "(b) [1pt] Give a setting of the weights and bias that correctly classi\ufb01es all the training examples. You don\u2019t need to show your work, but it might help you get partial credit.",
        "6",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "8. [2pts] Recall that in bagging, we compute an average of the predictions yavg = 1 m (cid:80)m",
        "7",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version A",
        "9. [2pts] Consider a regression problem where the input is a scalar x. Suppose we know that the dataset is generated by the following process. First, the target t is chosen from {0, 1} with equal probability. If t = 0, then x is sampled from a uniform distribution over the interval [1, 2]. If t = 1, then x is sampled from a uniform distribution over the interval [0, 2]. Give a function f (x), de\ufb01ned for x \u2208 [0, 2], such that y\u2217 = f (x) is the Bayes optimal predictor for t given x. (Note that even though t is binary valued, this is a regression problem, with squared error loss.)",
        "8",
        "CSC411/2515 Fall 2018",
        "(Scratch work or continued answers)",
        "9",
        "Midterm Test, Version A"
    ],
    "CSC411f18_midterm1": [
        "Midterm for CSC411/2515, Machine Learning and Data Mining Fall 2018, Version B Thursday, October 18, 8:10-9pm",
        "Name:",
        "Student number:",
        "This is a closed-book test. It is marked out of 15 marks. Please answer",
        "ALL of the questions. Here is some advice:",
        "The questions are NOT arranged in order of di\ufb03culty, so you should attempt every question.",
        "Questions that ask you to \u201cbrie\ufb02y explain\u201d something only require short (1-3 sentence) explanations. Don\u2019t write a full page of text. We\u2019re just looking for the main idea.",
        "None of the questions require long derivations. If you \ufb01nd yourself plug- ging through lots of equations, consider giving less detail or moving on to the next question.",
        "Many questions have more than one right answer.",
        "CSC411/2515 Fall 2018",
        ".",
        "Q1: Q2: Q3: Q4: Q5: Q6: Q7: Q8: Q9:",
        "Final mark:",
        "2",
        "/ 2 / 1 / 1 / 2 / 1 / 2 / 2 / 2 / 2",
        "/ 15",
        "Midterm Test, Version B",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "Have you taken CSC321 at UofT? (This question is used for calibration purposes.)",
        "1. As discussed in lecture, when applying K-nearest-neighbors, it is common to normalize each input dimension to unit variance.",
        "(a) [1pt] Why might it be advantageous to do this?",
        "(b) [1pt] When might this normalization step not be a good idea? (Hint: You may want to consider the task of classifying images of handwritten digits, where the digit is centered within the image.)",
        "2. [1pt] In random forests, what is the motivation for randomizing the set of attributes considered for each split?",
        "3",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "3. [1pt] Suppose you want to evaluate the test error rate of a 1-nearest-neighbors classi\ufb01er. Assume you implement the algorithm the na\u00a8\u0131ve way, i.e. by explicitly computing all the distances and taking the min, rather than by using a fancy data structure. What is the running time of evaluating the test error? Give your answer in big-O notation, in terms of the number of training examples Ntrain, the number of test examples Ntest, and the input dimension D. Brie\ufb02y explain your answer.",
        "4.",
        "(a) [1pt] Give one advantage of K-nearest-neighbors over linear regression.",
        "(b) [1pt] Give one advantage of linear regression over K-nearest-neighbors.",
        "4",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "5. [1pt] Suppose linear regression (with squared error loss) is used as a classi\ufb01cation algorithm. TRUE or FALSE: if it correctly classi\ufb01es every training example, then its cost is zero. (By \u201ccost\u201d, we mean the function minimized during training.) Brie\ufb02y justify your answer.",
        "6. [2pts] Let Z be a random variable and t be a real number. Show that",
        "E[(Z \u2212 t)2] = (E[Z] \u2212 t)2 + Var[Z].",
        "(This is a simpli\ufb01ed verison of the bias-variance decomposition.)",
        "5",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "7. [2pts] Suppose binary-valued random variables X and Y have the following joint distribution:",
        "Y = 0 Y = 1",
        "X = 0 X = 1",
        "1/8 2/8",
        "3/8 2/8",
        "Determine the information gain IG(Y |X). You may write your answer as a sum of logarithms.",
        "6",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "8. [2pts] Recall that combining the logistic activation function with squared error loss su\ufb00ers from saturation, whereby the gradient signal is very small when the prediction for a training example is very wrong. Logistic regression (i.e. logistic activation function with cross-entropy loss) doesn\u2019t have this problem. Recall that the logistic function is de\ufb01ned as \u03c3(z) = 1/(1 + e\u2212z). Now suppose we modify the activation function to squash the prediction y to be in the interval [0.1, 0.9], and then apply cross-entropy loss. I.e., z = w(cid:62)x + b y = 0.8\u03c3(z) + 0.1",
        "L(y, t) = \u2212t log y \u2212 (1 \u2212 t) log(1 \u2212 y),",
        "where \u03c3 is the logistic activation function. Does this model have a problem with saturation? You don\u2019t need to give a formal proof, but you should informally justify your answer. Hint: it is possible to answer this question without calculating derivatives. Think qualitatively.",
        "7",
        "CSC411/2515 Fall 2018",
        "Midterm Test, Version B",
        "9. [2pts] Recall that the soft-margin SVM can be viewed as minimizing the hinge loss with an L2 regularization term. I.e.,",
        "z = w(cid:62)x + b L(z, t) = max(0, 1 \u2212 tz)",
        "J (w, b) = \u03bb",
        "2 (cid:107)w(cid:107)2 +",
        "1 N",
        "N (cid:88)",
        "i=1",
        "L(z(i), t(i)).",
        "Here, t \u2208 {\u22121, +1}. Complete the formulas for the gradient calculations. You don\u2019t need to show your work.",
        "\u2202J \u2202w",
        "=",
        "+",
        "1 N",
        "N (cid:88)",
        "i=1",
        "\u2202L(i) \u2202w",
        "(\ufb01ll in the blank)",
        "dL dz",
        "=",
        "\u2202L \u2202w",
        "=",
        "(give in terms of",
        "dL dz",
        ")",
        "8",
        "CSC411/2515 Fall 2018",
        "(Scratch work or continued answers)",
        "9",
        "Midterm Test, Version B"
    ]
}