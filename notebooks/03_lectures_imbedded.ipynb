{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the DB is stored locally:\n",
    "\n",
    "When Embedded Weaviate starts for the first time, it creates a permanent datastore in the location set in your persistence_data_path. When your client exits, the Embedded Weaviate instance also exits, but the data persists . The next time the client runs, it starts a new instance of Embedded Weaviate. New Embedded Weaviate instances use the data that is saved in the datastore.\n",
    "\n",
    "## Data storage directory\n",
    "\n",
    "If XDG_DATA_HOME is set, the default is: XDG_DATA_HOME/weaviate/\n",
    "\n",
    "If XDG_DATA_HOME is not set, the default is: ~/.local/share/weaviate\n",
    "\n",
    "In my case the data is stored in the following location: /Users/username/.local/share/weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\"X-HuggingFace-Api-Key\": \"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\"}\n",
    "    )\n",
    ")\n",
    "assert client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the structure of the data vector dabase: We called it PDF_Document. This is the \"Class\" that we are going to use to store the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard pdf_document_HuFNascIdOa9 in 3.401316ms\",\"time\":\"2024-03-11T20:58:43-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-11T20:58:43-04:00\",\"took\":94044}\n"
     ]
    }
   ],
   "source": [
    "client.schema.delete_all()\n",
    "# Create a new class with a vectorizer\n",
    "schema = {\n",
    "    \"class\": \"PDF_Document\",    \n",
    "    \"vectorizer\": \"text2vec-huggingface\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"content\",  #What we want to vectorize\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"Content of PDF\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-huggingface\": {\"skip\": False, \"vectorizePropertyName\": False}\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"filename\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"PDF filename\"\n",
    "        },\n",
    "    ],\n",
    "    \"moduleConfig\": {\n",
    "    \"text2vec-huggingface\": {\n",
    "      \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Can be any public or private Hugging Face model.\n",
    "      \"options\": {\n",
    "        \"waitForModel\": True,  # Try this if you get a \"model not ready\" error\n",
    "      }\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "client.schema.create_class(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import DataSourceMetadata\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from weaviate.util import generate_uuid5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(elements, chunk_under_n_chars=500, chunk_new_after_n_chars=1500):\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        multipage_sections=False, # If True, the title of the first page is used for all pages\n",
    "        combine_text_under_n_chars=chunk_under_n_chars,\n",
    "        new_after_n_chars=chunk_new_after_n_chars\n",
    " \n",
    "    )\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        chunks[i] = {\"text\": chunks[i].text, \"filename\": chunks[i].metadata.filename}\n",
    "\n",
    "    chunk_texts = [x['text'] for x in chunks]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def add_data_to_weaviate(files, client, chunk_under_n_chars=500, chunk_new_after_n_chars=1500):\n",
    "    for filename in files:\n",
    "        try:\n",
    "            elements = partition_pdf(filename=filename)\n",
    "            chunks = get_chunks(elements, chunk_under_n_chars, chunk_new_after_n_chars)\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        print(f\"Uploading {len(chunks)} chunks for {str(filename)}.\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                client.data_object.create(class_name=\"PDF_Document\", data_object={\"content\": chunk['text'], \"filename\": filename})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Failed to upload chunk {i} for {str(filename)}.\")\n",
    "\n",
    "        with client.batch as batch:\n",
    "            for data_object in chunks:\n",
    "                batch.add_data_object(data_object={\"content\": chunk['text'], \"filename\": filename}, class_name=\"PDF_Document\", uuid=generate_uuid5(data_object))\n",
    "\n",
    "        \n",
    "    client.batch.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "def configure_batch(client: Client, batch_size: int, batch_target_rate: int):\n",
    "    \"\"\"\n",
    "    Configure the weaviate client's batch so it creates objects at `batch_target_rate`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    client : Client\n",
    "        The Weaviate client instance.\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "    batch_target_rate : int\n",
    "        The batch target rate as # of objects per second.\n",
    "    \"\"\"\n",
    "\n",
    "    def callback(batch_results: dict) -> None:\n",
    "\n",
    "        # you could print batch errors here\n",
    "        time_took_to_create_batch = batch_size * (client.batch.creation_time/client.batch.recommended_num_objects)\n",
    "        time.sleep(\n",
    "            max(batch_size/batch_target_rate - time_took_to_create_batch + 1, 0)\n",
    "        )\n",
    "\n",
    "    client.batch.configure(\n",
    "        batch_size=batch_size,\n",
    "        timeout_retries=5,\n",
    "        callback=callback,\n",
    "    )\n",
    "\n",
    "def add_data_to_weaviate(files, client, chunk_under_n_chars=500, chunk_new_after_n_chars=1500, batch_size=10, batch_target_rate=2):\n",
    "    configure_batch(client, batch_size, batch_target_rate)\n",
    "\n",
    "    for filename in files:\n",
    "        try:\n",
    "            elements = partition_pdf(filename=filename)\n",
    "            chunks = get_chunks(elements, chunk_under_n_chars, chunk_new_after_n_chars)\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        print(f\"Uploading {len(chunks)} chunks for {str(filename)}.\")\n",
    "        with client.batch as batch:\n",
    "            for chunk in chunks:\n",
    "                data_object = {\"content\": chunk['text'], \"filename\": filename}\n",
    "                batch.add_data_object(data_object=data_object, class_name=\"PDF_Document\", uuid=uuid.uuid5(uuid.NAMESPACE_DNS, str(data_object)))\n",
    "\n",
    "    client.batch.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the files to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 151 chunks for ../data/coursematerial/lec06.pdf.\n",
      "Uploading 123 chunks for ../data/coursematerial/lec12.pdf.\n",
      "Uploading 59 chunks for ../data/coursematerial/lec07.pdf.\n",
      "Uploading 60 chunks for ../data/coursematerial/lec11.pdf.\n",
      "Uploading 87 chunks for ../data/coursematerial/lec05.pdf.\n",
      "Uploading 215 chunks for ../data/coursematerial/lec04.pdf.\n",
      "Uploading 63 chunks for ../data/coursematerial/lec10.pdf.\n",
      "Uploading 77 chunks for ../data/coursematerial/lec01.pdf.\n",
      "Uploading 90 chunks for ../data/coursematerial/lec03.pdf.\n",
      "Uploading 67 chunks for ../data/coursematerial/lec02.pdf.\n",
      "Uploading 67 chunks for ../data/coursematerial/lec09.pdf.\n",
      "Uploading 87 chunks for ../data/coursematerial/lec08.pdf.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory_path = '../data/coursematerial/'\n",
    "import glob\n",
    "# Dictionary to hold file names and their elements\n",
    "\n",
    "# Find all PDF files in the specified directory\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '*.pdf'))\n",
    "add_data_to_weaviate(\n",
    "    files=pdf_files,\n",
    "    client=client,\n",
    "    chunk_under_n_chars=250,\n",
    "    chunk_new_after_n_chars=500,\n",
    "    batch_size=10,\n",
    "    batch_target_rate=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells below are two examples of queries to the database to get the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Get': {'PDF_Document': [{'content': 'Separating Hyperplanes\\n\\nSuppose we are given these data points from two diﬀerent classes and want to ﬁnd a linear classiﬁer that separates them.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec6\\n\\n5 / 44\\n\\nSeparating Hyperplanes', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'sha1_base64=\"19E7QQ3SzuFSITLhYdfln84cJaQ=\">AAACQnicdVDNTsJAGNzFP8Q/0KOXRqLxRFoueiRy8YiJgAk0ZLvdlpX9aXa3JqThHbzq8/gSvoI349WDS+lBIEzyJZOZb5LJBAmj2rjuJyxtbe/s7pX3KweHR8cn1dppT8tUYdLFkkn1FCBNGBWka6hh5ClRBPGAkX4wac/9/gtRmkrxaKYJ8TmKBY0oRsZKPY5UTMWoWncbbg5nnXgFqYMCnVENXg1DiVNOhMEMaT3w3MT4GVKGYkZmlWGqSYLwBMVkYKlAnGg/y+vOnEurhE4klT1hnFz9n8gQ13rKA/vJkRnrVW8ubvLMmM+WNRZLRa1M8QZjpa2Jbv2MiiQ1ROBF2ShljpHOfD8npIpgw6aWIGzzFDt4jBTCxq5cGebBrC05RyLUM7ust7rjOuk1G57b8B6a9dZdsXEZnIMLcA08cANa4B50QBdg8AxewRt4hx/wC37Dn', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'z = w(cid:62)x + b y = sign(z)\\n\\nThis is an equivalent formulation of binary linear classiﬁcation.\\n\\nLast week we considered how to get any w and b that minimized the cost on the training set.\\n\\nQuestion: How should we choose w and b to get the best generalization?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec6\\n\\n4 / 44', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'The decision boundary looks like a line because x as a D\\n\\n1 dimensional hyperplane.\\n\\n−\\n\\nRecall that a hyperplane is described by points x f (x) = w(cid:62)x + b = 0.\\n\\n∈\\n\\n∈\\n\\nR2, but think about it\\n\\nRD such that\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec6\\n\\n6 / 44\\n\\nSeparating Hyperplanes', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'aF39IE+vS/vJ+N+yfQ0480yB2gOmdwvg0SzLA==</latexit>', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'Very Important Announcement\\n\\nDespite my previous announcement, you do not have to be in a group for HW2.\\n\\nI have extended the HW2 deadline by 24 hours for everyone to make up for my mess.\\n\\nPlease double check your submission to avoid confusion. Some students have been overwriting other’s work, so check that your submission is as expected. I won’t penalize students based on this issue, but please please please check your submission so that there are no surprises in a few weeks.', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'sha1_base64=\"CenO+DINbFRCOV26HhAJh/UjCUs=\">AAACTnicdVBLS0JBGJ1rL7OX1rLNkBRBIPdGUJtActPSIB+gJnPHUYfmcZn5riUX/0nb+j1t+yPtosbHIhUPfHA45ztwOGEkuAXf//JSa+sbm1vp7czO7t7+QTZ3WLU6NpRVqBba1ENimeCKVYCDYPXIMCJDwWrhc2ns1wbMWK7VIwwj1pKkp3iXUwJOamezIb7AL09N0BF+xbfYb2fzfsGfAC+TYEbyaIZyO+edNTuaxpIpoIJY2wj8CFoJMcCpYKNMM7YsIvSZ9FjDUUUks61kUn2ET53SwV1t3CnAE/V/IiHS2qEM3ack0LeL3lhc5UFfjuY10dOGO5nTFcZCW+jetBKuohiYotOy3Vhg0Hi8Je5wwyiIoSOEujynmPaJIRTc4pnmJJiUtJREdezILRss7rhMqpeFwC8ED1f54t1s4zQ6RifoHAXoGhXRPSqjCqJogN7QO', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'If you still need to submit as a group and there are no groups, email me.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec6\\n\\n2 / 44\\n\\nToday\\n\\nSupport vector machines, elegant binary linear classiﬁers that generalize very well. Multiclass classiﬁcation: predicting a discrete(> 2)-valued target. Stochastic gradient descent, which lets us scale up gradient descent to large data sets.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec6\\n\\n3 / 44', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': '<latexit', 'filename': '../data/coursematerial/lec06.pdf'}, {'content': 'Binary Classiﬁcation with a Linear Model (Small Change)\\n\\nBinary classiﬁcation: predicting a target with two values\\n\\nTargets (small change from last week): t\\n\\n∈ {−\\n\\n1, +1 }\\n\\nLinear model (small change from last week):', 'filename': '../data/coursematerial/lec06.pdf'}]}}}\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\"http://localhost:8079\")\n",
    "# Perform a query\n",
    "query1 = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    PDF_Document (limit: 10) {\n",
    "      content\n",
    "      filename\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "result = client.query.raw(query1)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Aggregate': {'PDF_Document': [{'meta': {'count': 17}}]}}}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"\"\"\n",
    "{\n",
    "  Aggregate {\n",
    "    PDF_Document {\n",
    "      meta {\n",
    "        count\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "result = client.query.raw(query2)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
