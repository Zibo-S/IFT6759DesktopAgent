Midterm for CSC411/2515, Machine Learning and Data Mining Fall 2018, Version B Thursday, October 18, 8:10-9pm
Name:
Student number:
This is a closed-book test. It is marked out of 15 marks. Please answer
ALL of the questions. Here is some advice:
The questions are NOT arranged in order of diﬃculty, so you should attempt every question.
Questions that ask you to “brieﬂy explain” something only require short (1-3 sentence) explanations. Don’t write a full page of text. We’re just looking for the main idea.
None of the questions require long derivations. If you ﬁnd yourself plug- ging through lots of equations, consider giving less detail or moving on to the next question.
Many questions have more than one right answer.
CSC411/2515 Fall 2018
.
Q1: Q2: Q3: Q4: Q5: Q6: Q7: Q8: Q9:
Final mark:
2
/ 2 / 1 / 1 / 2 / 1 / 2 / 2 / 2 / 2
/ 15
Midterm Test, Version B
CSC411/2515 Fall 2018
Midterm Test, Version B
Have you taken CSC321 at UofT? (This question is used for calibration purposes.)
1. As discussed in lecture, when applying K-nearest-neighbors, it is common to normalize each input dimension to unit variance.
(a) [1pt] Why might it be advantageous to do this?
(b) [1pt] When might this normalization step not be a good idea? (Hint: You may want to consider the task of classifying images of handwritten digits, where the digit is centered within the image.)
2. [1pt] In random forests, what is the motivation for randomizing the set of attributes considered for each split?
3
CSC411/2515 Fall 2018
Midterm Test, Version B
3. [1pt] Suppose you want to evaluate the test error rate of a 1-nearest-neighbors classiﬁer. Assume you implement the algorithm the na¨ıve way, i.e. by explicitly computing all the distances and taking the min, rather than by using a fancy data structure. What is the running time of evaluating the test error? Give your answer in big-O notation, in terms of the number of training examples Ntrain, the number of test examples Ntest, and the input dimension D. Brieﬂy explain your answer.
4.
(a) [1pt] Give one advantage of K-nearest-neighbors over linear regression.
(b) [1pt] Give one advantage of linear regression over K-nearest-neighbors.
4
CSC411/2515 Fall 2018
Midterm Test, Version B
5. [1pt] Suppose linear regression (with squared error loss) is used as a classiﬁcation algorithm. TRUE or FALSE: if it correctly classiﬁes every training example, then its cost is zero. (By “cost”, we mean the function minimized during training.) Brieﬂy justify your answer.
6. [2pts] Let Z be a random variable and t be a real number. Show that
E[(Z − t)2] = (E[Z] − t)2 + Var[Z].
(This is a simpliﬁed verison of the bias-variance decomposition.)
5
CSC411/2515 Fall 2018
Midterm Test, Version B
7. [2pts] Suppose binary-valued random variables X and Y have the following joint distribution:
Y = 0 Y = 1
X = 0 X = 1
1/8 2/8
3/8 2/8
Determine the information gain IG(Y |X). You may write your answer as a sum of logarithms.
6
CSC411/2515 Fall 2018
Midterm Test, Version B
8. [2pts] Recall that combining the logistic activation function with squared error loss suﬀers from saturation, whereby the gradient signal is very small when the prediction for a training example is very wrong. Logistic regression (i.e. logistic activation function with cross-entropy loss) doesn’t have this problem. Recall that the logistic function is deﬁned as σ(z) = 1/(1 + e−z). Now suppose we modify the activation function to squash the prediction y to be in the interval [0.1, 0.9], and then apply cross-entropy loss. I.e., z = w(cid:62)x + b y = 0.8σ(z) + 0.1
L(y, t) = −t log y − (1 − t) log(1 − y),
where σ is the logistic activation function. Does this model have a problem with saturation? You don’t need to give a formal proof, but you should informally justify your answer. Hint: it is possible to answer this question without calculating derivatives. Think qualitatively.
7
CSC411/2515 Fall 2018
Midterm Test, Version B
9. [2pts] Recall that the soft-margin SVM can be viewed as minimizing the hinge loss with an L2 regularization term. I.e.,
z = w(cid:62)x + b L(z, t) = max(0, 1 − tz)
J (w, b) = λ
2 (cid:107)w(cid:107)2 +
1 N
N (cid:88)
i=1
L(z(i), t(i)).
Here, t ∈ {−1, +1}. Complete the formulas for the gradient calculations. You don’t need to show your work.
∂J ∂w
=
+
1 N
N (cid:88)
i=1
∂L(i) ∂w
(ﬁll in the blank)
dL dz
=
∂L ∂w
=
(give in terms of
dL dz
)
8
CSC411/2515 Fall 2018
(Scratch work or continued answers)
9
Midterm Test, Version B